{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d409d2a5-20c6-4e9e-9562-6961753a56e2",
   "metadata": {},
   "source": [
    "# Bed Reflectivity Calculation\n",
    "\n",
    "Now, take the raw bed-echo power, calculated in ./get-bed-power.ipynb, and use that to calculate a more 'physical' field for the bed reflectivity. This includes levelling the power at crossover points and correcting for spreading and attenuation losses.\n",
    "\n",
    "We also sort the traces by location into 'grounded', 'ungrounded', and by the known subglacial lakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2237c5d-c458-4329-8188-381285596c4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Standard Imports ###\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45b75c0-9aab-465d-979d-4c6fa1983dcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Get the bed pick dataframe ###\n",
    "\n",
    "# If the previous notebook has not been run to get bed-echo power download it from Zenodo\n",
    "fn = './Picked_Bed_Power.csv'\n",
    "if not os.path.exists(fn):\n",
    "    os.system('wget -L https://zenodo.org/records/11201199/files/Picked_Bed_Power.csv')\n",
    "# Read as pandas dataframe\n",
    "df = pd.read_csv('./Picked_Bed_Power.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c84f99-95f3-4d0c-887f-640d5291df13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Filter the dataset to Whillans Ice Plain only ###\n",
    "\n",
    "# All flight days\n",
    "fdays = np.sort(df['Flight Day'].unique())\n",
    "\n",
    "# Kamb flight days\n",
    "kamb_days = fdays[[1,3,4,7]]\n",
    "# Whillans flight days\n",
    "whillans_days = np.delete(fdays,[1,3,4,7])\n",
    "\n",
    "# Separate dataframes\n",
    "kamb_df = df[df['Flight Day'].isin(kamb_days)]\n",
    "df = df[df['Flight Day'].isin(whillans_days)]\n",
    "# Add in a single frame from a kamb day where part of the flight was over SLW\n",
    "df = pd.concat([df, kamb_df[(kamb_df['Flight Day']==20131227) & (kamb_df['Segment'] == 12)]])\n",
    "whillans_days = np.append(whillans_days,20131227)\n",
    "\n",
    "# Subset Whillans dataframe by y coordinate\n",
    "df = df[df['Y'] > -1e6]\n",
    "df = df[df['Y'] < -.48e6]\n",
    "\n",
    "# Remove traces with large aircraft pitch or roll\n",
    "df = df[abs(df['Roll']) < 0.1]\n",
    "df = df[abs(df['Pitch']) < 0.075]\n",
    "\n",
    "# Empty array for adjusted bed power\n",
    "df['P_bed_adj'] = df['P_bed'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c520908-d351-4f2a-9f5a-a0c49d9721e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### A function to level the power at crossover points ###\n",
    "\n",
    "def crossover_leveling(df,cal_variable,days,days_to_add,crossover_dist=100,print_num=5000,verbose=False):\n",
    "    \n",
    "    # Separate into two dataframes, 1st the 'true' power and 2nd to adjust\n",
    "    idxs1 = df['Flight Day'].isin(days)\n",
    "    idxs2 = df['Flight Day'].isin(days_to_add)\n",
    "    df1 = df.loc[idxs1]\n",
    "    df2 = df.loc[idxs2]\n",
    "    \n",
    "    # Empty array for power differences at crossovers\n",
    "    dP = np.array([])\n",
    "    \n",
    "    # Loop through every trace in the 2nd dataframe\n",
    "    for i,idx in enumerate(df2.index):\n",
    "        if i%print_num==0 and verbose:\n",
    "            print(round(i/len(df2),3)*100,'% finished...')\n",
    "            print('Total crossover traces added:',len(dP))\n",
    "        # Get the distance from the selected trace to every trace in df1\n",
    "        dist = np.sqrt((df2.loc[idx,'X']-df1['X'])**2.+(df2.loc[idx,'Y']-df1['Y'])**2.)\n",
    "        # Add the power difference for \n",
    "        dP = np.append(dP,df1[cal_variable][dist<crossover_dist] - df2.loc[idx,cal_variable])\n",
    "\n",
    "    # Adjust the power for the added days\n",
    "    df.loc[idxs2,'P_bed_adj'] = df.loc[idxs2,'P_bed'] + np.nanmean(dP)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Power adjusted by:\",np.nanmean(dP),' for days:',days_to_add)\n",
    "        \n",
    "    return df, dP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bc17e0-cdbb-462e-841a-fde5d2586a61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Do the crossover power adjustment for each day ###\n",
    "\n",
    "crossover_variable = 'P_surf'\n",
    "\n",
    "# Initialize the figures and colors to plot for each day\n",
    "plt.figure()\n",
    "prop_cycle = plt.rcParams['axes.prop_cycle']\n",
    "colors = prop_cycle.by_key()['color']\n",
    "\n",
    "for i in range(1,9):\n",
    "    # Do the adjustment for each day, using the function from the previous cell\n",
    "    print(\"Crossover Leveling Adjustment #\",i)\n",
    "    print(\"Control Days:\",np.delete(whillans_days,i))\n",
    "    print(\"Adjusted Days:\",whillans_days[i])\n",
    "    df, dP = crossover_leveling(df,crossover_variable,np.delete(whillans_days,i),[whillans_days[i]],crossover_dist=50,print_num=10000,verbose=True)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Plot a histogram of power difference at crossover points\n",
    "    plt.hist(-dP,bins=len(dP)//100,color=colors[i],alpha=0.25,label=whillans_days[i])\n",
    "    plt.axvline(-np.nanmean(dP),color=colors[i])\n",
    "\n",
    "# Save a variable for the adjusted surface power\n",
    "df['P_surf_adj'] = df['P_surf'] + (df['P_bed_adj'] - df['P_bed'])\n",
    "\n",
    "plt.ylim(0,1000)\n",
    "plt.xlabel('Crossover Power (dB)')\n",
    "plt.ylabel('Number of Traces')\n",
    "plt.legend(title='Flight Day',fontsize=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05225f46-689a-4bf4-a8d1-2c16bbc1aac2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Plot the adjusted power by day ###\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "# Map view plot\n",
    "ax1 = plt.subplot(121)\n",
    "for i in range(9):\n",
    "    df[df['Flight Day']==whillans_days[i]].plot.scatter(ax=ax1,x='X',y='Y',c=colors[i],s=1,label=whillans_days[i])\n",
    "\n",
    "# Histogram\n",
    "ax2 = plt.subplot(122)\n",
    "bins = np.arange(-160,-70,0.1)\n",
    "plt.hist(df['P_bed_adj'],bins=bins,color='lightgrey');\n",
    "for i,day in enumerate(whillans_days):\n",
    "    plt.hist(df[df['Flight Day']==day]['P_bed_adj'],bins=bins,color=colors[i],label=day)\n",
    "plt.legend(title='Flight Day',fontsize=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5600edaa-49f3-47c2-90d0-b2815b4d09a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Calculate a relative reflectivity ###\n",
    "\n",
    "# ice permittivity\n",
    "epsr_ice = 3.15\n",
    "# geometric spreading losses (spherical)\n",
    "G = 20*np.log10(2.*(df['h'] + df['H']/np.sqrt(epsr_ice)))\n",
    "\n",
    "# Attenuation correction\n",
    "N = 10                   # one-way rate (dB/km)\n",
    "L = 2.*N * df['H']/1000. # total attenuative losses\n",
    "\n",
    "# Reflectivity\n",
    "df['Relative Reflectivity'] = df['P_bed_adj'] + G + L\n",
    "# Move the mean to 0\n",
    "df['Relative Reflectivity'] -= df['Relative Reflectivity'].mean()\n",
    "\n",
    "### Plot the output reflectivity field ###\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "\n",
    "ax1 = plt.subplot(111)\n",
    "df.plot.scatter(ax=ax1,x='X',y='Y',c='Relative Reflectivity',cmap='magma',s=1,vmin=-25,vmax=15)\n",
    "ax1.axis('equal')\n",
    "plt.xlim(-300000,-100000)\n",
    "plt.ylim(-800000,-500000)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c4b70c-68af-4ec2-8205-49174d524541",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Import Scripps grounding line and create a mask for traces grounded/floating ###\n",
    "\n",
    "import json\n",
    "from osgeo import ogr\n",
    "import matplotlib.path as mplPath\n",
    "import geopandas as gpd\n",
    "\n",
    "# a base directory where I keep data to be loaded below\n",
    "data_dir = os.getenv('DATAHOME')\n",
    "# quick error check to make sure $DATAHOME is set\n",
    "if data_dir is None:\n",
    "    raise OSError('environment variable $DATAHOME does not exist')\n",
    "\n",
    "# Scripps Grounding Line\n",
    "gl = data_dir + 'continental-data-products/antarctica/grounding-line/scripps/scripps_antarctica_polygons_v1.shp' \n",
    "gl_df = gpd.read_file(gl)\n",
    "gl_coords = np.transpose(gl_df.iloc[1010].geometry.exterior.coords.xy)\n",
    "\n",
    "# Crop to the Whillans Ice Plain region\n",
    "start = 46000\n",
    "end = start+3000\n",
    "gl_crop = gl_coords[start:end]\n",
    "gl_crop = np.append(gl_crop,np.array([[-1000000,0],[1000000,0],gl_crop[0]]),axis=0)\n",
    "\n",
    "# Label the radar points as grounded or not\n",
    "gl_Path = mplPath.Path(np.transpose([gl_crop[:,0],gl_crop[:,1]]))\n",
    "df['Grounded'] = gl_Path.contains_points(df[['X','Y']]).astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e93100b-16a3-4cb1-962a-9857a878b460",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Import lake outlines and create a mask for traces lake/not-lake ###\n",
    "\n",
    "import h5py\n",
    "\n",
    "# Subglacial lake outlines (Siegfried & Fricker, 2018)\n",
    "lakes = h5py.File('../data/outlines/SiegfriedFricker2018-outlines.h5', 'r')\n",
    "wh_lake_keys = ['EngelhardtSubglacialLake', 'WhillansSubglacialLake', 'Whillans_6', 'Whillans_7', 'Whillans_8','Lake78','Lake10']\n",
    "\n",
    "# empty column to be filled\n",
    "df['Lake'] = np.zeros_like(df['X']).astype(bool)\n",
    "# loop through all lake keys\n",
    "for key in wh_lake_keys:\n",
    "    lake_x = np.squeeze(lakes[key]['x'][:])\n",
    "    lake_y = np.squeeze(lakes[key]['y'][:])\n",
    "    lake_Path = mplPath.Path(np.transpose([lake_x,lake_y]))\n",
    "    df[key] = lake_Path.contains_points(df[['X','Y']]).astype(bool)\n",
    "    df['Lake'] = np.logical_or(df['Lake'],df[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64492f18-1e7d-44d5-834f-95846a21c8b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Plot the relative reflectivity from each of the spatially sorted fields ###\n",
    "\n",
    "plt.figure(figsize=(12,3))\n",
    "\n",
    "# Grounded\n",
    "ax1 = plt.subplot(131)\n",
    "df[df['Grounded']==True].plot.scatter(ax=ax1,x='X',y='Y',c='Relative Reflectivity',cmap='magma',s=1,vmin=-25,vmax=15)\n",
    "plt.plot(gl_crop[:,0],gl_crop[:,1],'k-')\n",
    "ax1.axis('equal')\n",
    "plt.xlim(-300000,-100000)\n",
    "plt.ylim(-800000,-500000)\n",
    "\n",
    "# Subglacial Lake\n",
    "ax2 = plt.subplot(132)\n",
    "df[df['Lake']==True].plot.scatter(ax=ax2,x='X',y='Y',c='Relative Reflectivity',cmap='magma',s=1,vmin=-25,vmax=15)\n",
    "plt.plot(gl_crop[:,0],gl_crop[:,1],'k-')\n",
    "ax2.axis('equal')\n",
    "plt.xlim(-300000,-100000)\n",
    "plt.ylim(-800000,-500000)\n",
    "\n",
    "# Not Grounded (Ice Shelf)\n",
    "ax3 = plt.subplot(133)\n",
    "df[df['Grounded']==False].plot.scatter(ax=ax3,x='X',y='Y',c='Relative Reflectivity',cmap='magma',s=1,vmin=-25,vmax=15)\n",
    "plt.plot(gl_crop[:,0],gl_crop[:,1],'k-')\n",
    "ax3.axis('equal')\n",
    "plt.xlim(-300000,-100000)\n",
    "plt.ylim(-800000,-500000)\n",
    "\n",
    "# Plot lake outlines\n",
    "for key in lakes.keys():\n",
    "    lake_x = np.squeeze(lakes[key]['x'][:])\n",
    "    lake_y = np.squeeze(lakes[key]['y'][:])\n",
    "    for ax in [ax1,ax3]:\n",
    "        ax.plot(lake_x,lake_y,c='steelblue')\n",
    "        \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b4a502-fe85-4883-8d10-a8a5840f018c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Save the calculated reflectivity dataframe as a new csv file ###\n",
    "\n",
    "df.to_csv('./Processed_Reflectivity.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
